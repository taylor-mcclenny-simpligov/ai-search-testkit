# TRS-MVP-3 vs ROKO MVP (Batch Summary)

Doc-level metrics (top-5 overlap vs benchmark). TRS-MVP-3 uses a single combined text query against `vector_main`; ROKO aggregates chunk hits to top-5 unique parent docs (k=15 per vector).

| Batch | TRS-MVP-3 self@1 | TRS-MVP-3 avg overlap | TRS-MVP-3 avg query (s) | ROKO self@1 | ROKO avg overlap | ROKO avg query (s) |
| --- | --- | --- | --- | --- | --- | --- |
| 1 | 27/29 | 2.72 | ~0.99 | 27/29 | 2.59 | n/a (not timed this run) |
| 2 | 29/30 | 2.60 | ~0.91 | 27/30 | 2.63 | n/a (not timed this run) |
| 3 | 26/28 | 2.57 | ~0.98 | 24/28 | 2.50 | n/a (not timed this run) |
| 4 | 30/30 | 2.97 | ~0.98 | 26/30 | 2.70 | ~0.50 |
| 5 | 26/30 | 2.53 | ~1.02 | 26/30 | 2.47 | ~0.50 |
| 6 | 26/29* | 2.59 | ~0.94 | 25/30 | 2.53 | ~0.19 |
| 7 | 26/29† | 2.72 | ~0.96 | 23/29 | 2.60 | ~0.15 |
| **All** | 187/205 | 2.67 | ~1.00 | 178/206 | 2.57 | ~0.34 |

Quick takeaways
- TRS-MVP-3 latency stays ~0.9–1.0s/query vs TRS-MVP-2’s ~2.8s by using a single combined query.
- For batches 1–5, TRS-MVP-3 generally edges ROKO on self@1/overlap; batch 3 was ROKO-favorable on overlap.
- ROKO remains the fastest (~0.15–0.5s where measured) with chunk aggregation.

Follow-ups
- Time ROKO for batches 1-3 to complete the speed table.
- Per-parent deltas: identify patterns where chunked ROKO wins; consider query k alignment (TRS k=5 vs ROKO k=15->dedup).
- Token-budget heuristic is global (~7.5k) with headings->fields->semantics order; adjust if 502s persist.
- Time ROKO for batches 1–3 to complete the speed table.

Strengths / Weaknesses (ROKO vs TRS)
- ROKO strengths: much faster queries (small chunk vectors + short query text), resilient to long docs (no truncation), dual signals (semantic + layout), chunk-level matches can surface specific sections.
- ROKO weaknesses: requires chunk aggregation/dedup to parent; overlap can scatter; more complex pipeline; timing not logged for batches 1–3.
- TRS strengths: single-vector-per-doc search; simpler aggregation; strong on self@1 in many batches; combined text captures holistic context.
- TRS weaknesses: slower queries (large query embeds), truncation risk on very long texts, historical duplication/noise (mitigated by dedupe), single-vector may dilute section-level signals.

Brief system overview
- TRS: Index `simpligov-text-records` with single vectors per doc (full/main/meta). Runtime: PDF extractor `trs_extract.py` produces headings/fields/semantics; query combines them (headings->fields->semantics), dedupes headings vs fields, applies a semantics-first token budget, then a single text-vector search (k=5).
- ROKO: Index `simpligov-sgw-index` with chunk-level docs storing semantic + layout vectors. Query embeds short semantic/layout strings, runs two vector searches (k≈15 each), then dedupes to top parent docs; fast because queries/chunks are small.
- Benchmarks: Index `simpligov-sgws-compressed` stores compressed .sgws vectors; `benchmark.json` is generated by querying this index with each compressed .sgws for gold-standard comparisons.